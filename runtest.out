/home/reemha/.conda/envs/Privacy/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/home/reemha/.conda/envs/Privacy/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/home/reemha/.conda/envs/Privacy/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
You choose to run with:
Model: t5-base
Attacking?: 
The privatization will be applied on: Train+Test
Dataset: qnli
The privatization technique: conv
Sigma: 0.6
Saving results: finetune-t5-base-qnli
Training Model
13092 683

 Epoch 1 / 5
  Batch   500  of  13,092.
  Batch 1,000  of  13,092.
  Batch 1,500  of  13,092.
  Batch 2,000  of  13,092.
  Batch 2,500  of  13,092.
  Batch 3,000  of  13,092.
  Batch 3,500  of  13,092.
  Batch 4,000  of  13,092.
  Batch 4,500  of  13,092.
  Batch 5,000  of  13,092.
  Batch 5,500  of  13,092.
  Batch 6,000  of  13,092.
  Batch 6,500  of  13,092.
  Batch 7,000  of  13,092.
  Batch 7,500  of  13,092.
  Batch 8,000  of  13,092.
  Batch 8,500  of  13,092.
  Batch 9,000  of  13,092.
  Batch 9,500  of  13,092.
  Batch 10,000  of  13,092.
  Batch 10,500  of  13,092.
  Batch 11,000  of  13,092.
  Batch 11,500  of  13,092.
  Batch 12,000  of  13,092.
  Batch 12,500  of  13,092.
  Batch 13,000  of  13,092.
tensor(0.2034, device='cuda:0', grad_fn=<DivBackward0>)

Evaluating...
  Batch   100  of    683.
  Batch   200  of    683.
  Batch   300  of    683.
  Batch   400  of    683.
  Batch   500  of    683.
  Batch   600  of    683.

Training Loss: 0.203
Validation Loss: 0.149
Accuracy: 0.875

 Epoch 2 / 5
  Batch   500  of  13,092.
  Batch 1,000  of  13,092.
  Batch 1,500  of  13,092.
  Batch 2,000  of  13,092.
  Batch 2,500  of  13,092.
  Batch 3,000  of  13,092.
  Batch 3,500  of  13,092.
  Batch 4,000  of  13,092.
  Batch 4,500  of  13,092.
  Batch 5,000  of  13,092.
  Batch 5,500  of  13,092.
  Batch 6,000  of  13,092.
  Batch 6,500  of  13,092.
  Batch 7,000  of  13,092.
  Batch 7,500  of  13,092.
  Batch 8,000  of  13,092.
  Batch 8,500  of  13,092.
  Batch 9,000  of  13,092.
  Batch 9,500  of  13,092.
  Batch 10,000  of  13,092.
  Batch 10,500  of  13,092.
  Batch 11,000  of  13,092.
  Batch 11,500  of  13,092.
  Batch 12,000  of  13,092.
  Batch 12,500  of  13,092.
  Batch 13,000  of  13,092.
tensor(0.1376, device='cuda:0', grad_fn=<DivBackward0>)

Evaluating...
  Batch   100  of    683.
  Batch   200  of    683.
  Batch   300  of    683.
  Batch   400  of    683.
  Batch   500  of    683.
  Batch   600  of    683.

Training Loss: 0.138
Validation Loss: 0.139
Accuracy: 0.883

 Epoch 3 / 5
  Batch   500  of  13,092.
  Batch 1,000  of  13,092.
  Batch 1,500  of  13,092.
  Batch 2,000  of  13,092.
  Batch 2,500  of  13,092.
  Batch 3,000  of  13,092.
  Batch 3,500  of  13,092.
  Batch 4,000  of  13,092.
  Batch 4,500  of  13,092.
  Batch 5,000  of  13,092.
  Batch 5,500  of  13,092.
  Batch 6,000  of  13,092.
  Batch 6,500  of  13,092.
  Batch 7,000  of  13,092.
  Batch 7,500  of  13,092.
  Batch 8,000  of  13,092.
  Batch 8,500  of  13,092.
  Batch 9,000  of  13,092.
  Batch 9,500  of  13,092.
  Batch 10,000  of  13,092.
  Batch 10,500  of  13,092.
  Batch 11,000  of  13,092.
  Batch 11,500  of  13,092.
  Batch 12,000  of  13,092.
  Batch 12,500  of  13,092.
  Batch 13,000  of  13,092.
tensor(0.1015, device='cuda:0', grad_fn=<DivBackward0>)

Evaluating...
  Batch   100  of    683.
  Batch   200  of    683.
  Batch   300  of    683.
  Batch   400  of    683.
  Batch   500  of    683.
  Batch   600  of    683.

Training Loss: 0.102
Validation Loss: 0.160
Accuracy: 0.871

 Epoch 4 / 5
  Batch   500  of  13,092.
  Batch 1,000  of  13,092.
  Batch 1,500  of  13,092.
  Batch 2,000  of  13,092.
  Batch 2,500  of  13,092.
  Batch 3,000  of  13,092.
  Batch 3,500  of  13,092.
  Batch 4,000  of  13,092.
  Batch 4,500  of  13,092.
  Batch 5,000  of  13,092.
  Batch 5,500  of  13,092.
  Batch 6,000  of  13,092.
  Batch 6,500  of  13,092.
  Batch 7,000  of  13,092.
  Batch 7,500  of  13,092.
  Batch 8,000  of  13,092.
  Batch 8,500  of  13,092.
  Batch 9,000  of  13,092.
  Batch 9,500  of  13,092.
  Batch 10,000  of  13,092.
  Batch 10,500  of  13,092.
  Batch 11,000  of  13,092.
  Batch 11,500  of  13,092.
  Batch 12,000  of  13,092.
  Batch 12,500  of  13,092.
  Batch 13,000  of  13,092.
tensor(0.0678, device='cuda:0', grad_fn=<DivBackward0>)

Evaluating...
  Batch   100  of    683.
  Batch   200  of    683.
  Batch   300  of    683.
  Batch   400  of    683.
  Batch   500  of    683.
  Batch   600  of    683.

Training Loss: 0.068
Validation Loss: 0.184
Accuracy: 0.874

 Epoch 5 / 5
  Batch   500  of  13,092.
  Batch 1,000  of  13,092.
  Batch 1,500  of  13,092.
  Batch 2,000  of  13,092.
  Batch 2,500  of  13,092.
  Batch 3,000  of  13,092.
  Batch 3,500  of  13,092.
  Batch 4,000  of  13,092.
  Batch 4,500  of  13,092.
  Batch 5,000  of  13,092.
  Batch 5,500  of  13,092.
  Batch 6,000  of  13,092.
  Batch 6,500  of  13,092.
  Batch 7,000  of  13,092.
  Batch 7,500  of  13,092.
  Batch 8,000  of  13,092.
  Batch 8,500  of  13,092.
  Batch 9,000  of  13,092.
  Batch 9,500  of  13,092.
  Batch 10,000  of  13,092.
  Batch 10,500  of  13,092.
  Batch 11,000  of  13,092.
  Batch 11,500  of  13,092.
  Batch 12,000  of  13,092.
  Batch 12,500  of  13,092.
  Batch 13,000  of  13,092.
tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)

Evaluating...
  Batch   100  of    683.
  Batch   200  of    683.
  Batch   300  of    683.
  Batch   400  of    683.
  Batch   500  of    683.
  Batch   600  of    683.

Training Loss: 0.061
Validation Loss: 0.190
Accuracy: 0.886
